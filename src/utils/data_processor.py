"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–ª–≥–æ—Ç—Ä–µ–π–¥–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã

–ü–†–û–ë–õ–ï–ú–´ –í –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ï:
1. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω
2. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
3. –í–æ–∑–º–æ–∂–Ω–∞—è —É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±—É–¥—É—â–µ–≥–æ
4. –ù–µ—Ç walk-forward validation

–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
1. –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
2. –°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
3. –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏
4. –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è walk-forward
"""

import pandas as pd
import numpy as np
from typing import Tuple, Dict, Any, Optional
import logging
from pathlib import Path

from ..core.config import DataConfig


class DataProcessor:
    """
    –£–õ–£–ß–®–ï–ù–ù–´–ô –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –ö—ç—à –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self._processed_cache: Dict[str, pd.DataFrame] = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
        self.data_stats: Dict[str, Any] = {}
    
    def load_and_validate_data(self, file_path: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –±–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        self.logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {file_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        try:
            df = pd.read_csv(file_path)
        except Exception as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        for col in required_columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ —á–∏—Å–ª–æ–≤–∞—è, –ø—ã—Ç–∞–µ–º—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å")
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ü–µ–Ω OHLC
        invalid_ohlc = (
            (df['high'] < df['low']) |
            (df['high'] < df['open']) |
            (df['high'] < df['close']) |
            (df['low'] > df['open']) |
            (df['low'] > df['close'])
        )
        
        if invalid_ohlc.any():
            self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ OHLC –¥–∞–Ω–Ω—ã–µ: {invalid_ohlc.sum()} —Å—Ç—Ä–æ–∫")
            df = df[~invalid_ohlc].reset_index(drop=True)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω—É–ª–µ–≤—ã–µ/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            invalid_prices = (df[col] <= 0)
            if invalid_prices.any():
                self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω—É–ª–µ–≤—ã–µ/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã –≤ {col}: {invalid_prices.sum()} —Å—Ç—Ä–æ–∫")
                df = df[~invalid_prices].reset_index(drop=True)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã timestamp (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if 'timestamp' in df.columns:
            duplicates = df.duplicated(subset=['timestamp'])
            if duplicates.any():
                self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫: {duplicates.sum()}")
                df = df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)
        
        self.logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        return df
    
    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –£–õ–£–ß–®–ï–ù–ù–´–ô —Ä–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ —É—Ç–µ—á–∫–∏
        """
        df = df.copy()
        
        self.logger.info("üîß –†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        # === –ë–ê–ó–û–í–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
        
        # EMA (Exponential Moving Average)
        df['ema_fast'] = df['close'].ewm(span=DataConfig.EMA_FAST_SPAN, adjust=False).mean()
        df['ema_slow'] = df['close'].ewm(span=DataConfig.EMA_SLOW_SPAN, adjust=False).mean()
        df['ema_signal'] = (df['ema_fast'] > df['ema_slow']).astype(int)
        
        # RSI (Relative Strength Index)
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=DataConfig.RSI_WINDOW).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=DataConfig.RSI_WINDOW).mean()
        rs = gain / (loss + 1e-8)  # –î–æ–±–∞–≤–ª—è–µ–º –º–∞–ª–æ–µ —á–∏—Å–ª–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
        df['rsi'] = 100 - (100 / (1 + rs))
        df['rsi_oversold'] = (df['rsi'] < DataConfig.RSI_OVERSOLD).astype(int)
        df['rsi_overbought'] = (df['rsi'] > DataConfig.RSI_OVERBOUGHT).astype(int)
        
        # MACD
        ema_12 = df['close'].ewm(span=DataConfig.MACD_FAST).mean()
        ema_26 = df['close'].ewm(span=DataConfig.MACD_SLOW).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        df['macd_bullish'] = (df['macd'] > df['macd_signal']).astype(int)
        
        # === –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ ===
        
        # Bollinger Bands
        bb_window = DataConfig.BOLLINGER_WINDOW
        df['bb_middle'] = df['close'].rolling(window=bb_window).mean()
        bb_std = df['close'].rolling(window=bb_window).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
        
        # Volume indicators
        df['volume_sma'] = df['volume'].rolling(window=20).mean()
        df['volume_ratio'] = df['volume'] / (df['volume_sma'] + 1e-8)
        
        # Price action
        df['price_change'] = df['close'].pct_change()
        df['log_return'] = np.log(df['close'] / df['close'].shift(1))
        df['volatility'] = df['log_return'].rolling(window=20).std()
        df['price_trend'] = df['close'].pct_change(periods=5)
        
        # ATR (Average True Range)
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        df['atr'] = true_range.rolling(window=14).mean()
        
        # Momentum indicators
        df['momentum'] = df['close'] / df['close'].shift(DataConfig.MOMENTUM_WINDOW) - 1
        df['roc'] = ((df['close'] - df['close'].shift(12)) / df['close'].shift(12)) * 100
        
        # === –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ –î–õ–Ø –ü–†–ò–ë–´–õ–¨–ù–û–°–¢–ò ===
        
        # Stochastic Oscillator
        low_14 = df['low'].rolling(window=14).min()
        high_14 = df['high'].rolling(window=14).max()
        df['stoch_k'] = 100 * ((df['close'] - low_14) / (high_14 - low_14 + 1e-8))
        df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()
        df['stoch_oversold'] = (df['stoch_k'] < 20).astype(int)
        df['stoch_overbought'] = (df['stoch_k'] > 80).astype(int)
        
        # Williams %R
        df['williams_r'] = -100 * ((high_14 - df['close']) / (high_14 - low_14 + 1e-8))
        
        # Commodity Channel Index (CCI)
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        sma_tp = typical_price.rolling(window=20).mean()
        mad = typical_price.rolling(window=20).apply(lambda x: np.abs(x - x.mean()).mean())
        df['cci'] = (typical_price - sma_tp) / (0.015 * mad + 1e-8)
        
        # Money Flow Index (MFI)
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']
        positive_mf = money_flow.where(typical_price.diff() > 0, 0).rolling(window=14).sum()
        negative_mf = money_flow.where(typical_price.diff() < 0, 0).rolling(window=14).sum()
        df['mfi'] = 100 - (100 / (1 + positive_mf / (negative_mf + 1e-8)))
        
        # On-Balance Volume (OBV)
        df['obv'] = (df['volume'] * np.where(df['close'].diff() > 0, 1, 
                                            np.where(df['close'].diff() < 0, -1, 0))).cumsum()
        df['obv_signal'] = (df['obv'] > df['obv'].rolling(window=20).mean()).astype(int)
        
        # Ichimoku Cloud components
        nine_period_high = df['high'].rolling(window=9).max()
        nine_period_low = df['low'].rolling(window=9).min()
        df['tenkan_sen'] = (nine_period_high + nine_period_low) / 2
        
        twenty_six_period_high = df['high'].rolling(window=26).max()
        twenty_six_period_low = df['low'].rolling(window=26).min()
        df['kijun_sen'] = (twenty_six_period_high + twenty_six_period_low) / 2
        
        df['senkou_span_a'] = ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26)
        fifty_two_period_high = df['high'].rolling(window=52).max()
        fifty_two_period_low = df['low'].rolling(window=52).min()
        df['senkou_span_b'] = ((fifty_two_period_high + fifty_two_period_low) / 2).shift(26)
        
        # Price position relative to Ichimoku cloud
        df['above_cloud'] = (df['close'] > df[['senkou_span_a', 'senkou_span_b']].max(axis=1)).astype(int)
        df['below_cloud'] = (df['close'] < df[['senkou_span_a', 'senkou_span_b']].min(axis=1)).astype(int)
        
        # Multi-timeframe analysis
        df['sma_5'] = df['close'].rolling(window=5).mean()
        df['sma_10'] = df['close'].rolling(window=10).mean()
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        
        # Trend strength indicators
        df['trend_strength'] = ((df['sma_5'] > df['sma_10']).astype(int) + 
                               (df['sma_10'] > df['sma_20']).astype(int) + 
                               (df['sma_20'] > df['sma_50']).astype(int))
        
        # Market sentiment indicators
        df['bullish_sentiment'] = ((df['rsi'] > 50).astype(int) + 
                                  (df['macd'] > df['macd_signal']).astype(int) + 
                                  (df['stoch_k'] > 50).astype(int) + 
                                  (df['above_cloud']).astype(int))
        
        df['bearish_sentiment'] = ((df['rsi'] < 50).astype(int) + 
                                  (df['macd'] < df['macd_signal']).astype(int) + 
                                  (df['stoch_k'] < 50).astype(int) + 
                                  (df['below_cloud']).astype(int))
        
        # Volatility regime detection
        df['volatility_regime'] = (df['volatility'] > df['volatility'].rolling(window=50).mean()).astype(int)
        
        # Price action patterns
        df['doji'] = (np.abs(df['close'] - df['open']) / (df['high'] - df['low'] + 1e-8) < 0.1).astype(int)
        df['hammer'] = ((df['close'] > df['open']) & 
                       ((df['close'] - df['open']) / (0.001 + df['close'] - df['open']) < 0.3) &
                       ((df['open'] - df['low']) / (0.001 + df['high'] - df['low']) > 0.6)).astype(int)
        
        # === –ü–†–û–í–ï–†–ö–ê –ù–ê –£–¢–ï–ß–ö–ò –î–ê–ù–ù–´–• ===
        self._validate_no_future_leakage(df)
        
        self.logger.info(f"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ {len([col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume']])} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        
        return df
    
    def _validate_no_future_leakage(self, df: pd.DataFrame):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±—É–¥—É—â–µ–≥–æ"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª—ã–µ –¥–∞–Ω–Ω—ã–µ
        # –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ backtesting
        
        indicators_to_check = ['ema_fast', 'ema_slow', 'rsi', 'macd', 'bb_middle']
        
        for indicator in indicators_to_check:
            if indicator in df.columns:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –±—É–¥—É—â–∏–º–∏ —Ü–µ–Ω–∞–º–∏
                future_correlation = df[indicator].shift(1).corr(df['close'].shift(-1))
                if abs(future_correlation) > 0.1:  # –ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    self.logger.warning(f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–∞—è —É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–µ {indicator}: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –±—É–¥—É—â–∏–º = {future_correlation:.3f}")
    
    def normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è - –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ü–µ–Ω—ã!
        """
        df = df.copy()
        
        self.logger.info("üîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –¶–µ–Ω—ã –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Ç–æ—Ä–≥–æ–≤–æ–π –ª–æ–≥–∏–∫–∏
        price_columns = ['open', 'high', 'low', 'close']
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ –æ–±—ä–µ–º
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        indicators_to_normalize = [col for col in numeric_columns 
                                 if col not in price_columns and col != 'volume_ratio']
        
        # Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        for col in indicators_to_normalize:
            if col in df.columns:
                mean_val = df[col].mean()
                std_val = df[col].std()
                if std_val > 1e-8:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∏—Å–ª–∞
                    df[col] = (df[col] - mean_val) / std_val
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    self.data_stats[f'{col}_mean'] = mean_val
                    self.data_stats[f'{col}_std'] = std_val
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–º–∞
        if 'volume' in df.columns:
            df['volume'] = np.log1p(df['volume'])  # log(1 + x) –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        
        self.logger.info("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—Ü–µ–Ω—ã –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
        return df
    
    def split_data_for_walk_forward(self, df: pd.DataFrame, 
                                  train_ratio: float = 0.8) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è walk-forward validation
        """
        split_idx = int(len(df) * train_ratio)
        
        train_df = df.iloc[:split_idx].copy()
        test_df = df.iloc[split_idx:].copy()
        
        self.logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: –æ–±—É—á–µ–Ω–∏–µ={len(train_df)}, —Ç–µ—Å—Ç={len(test_df)}")
        
        return train_df, test_df
    
    def prepare_data(self, file_path: str, use_cache: bool = True) -> pd.DataFrame:
        """
        –ì–õ–ê–í–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
        """
        cache_key = f"{file_path}_{DataConfig.WINDOW_SIZE}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and cache_key in self._processed_cache:
            self.logger.info("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return self._processed_cache[cache_key]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = self.load_and_validate_data(file_path)
        df = self.calculate_technical_indicators(df)
        df = self.normalize_features(df)
        
        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        initial_len = len(df)
        df = df.dropna().reset_index(drop=True)
        final_len = len(df)
        
        if initial_len != final_len:
            self.logger.info(f"üßπ –£–¥–∞–ª–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è: {initial_len} -> {final_len} —Å—Ç—Ä–æ–∫")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        if len(df) < DataConfig.WINDOW_SIZE + 100:
            raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(df)} < {DataConfig.WINDOW_SIZE + 100}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if use_cache:
            self._processed_cache[cache_key] = df
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.data_stats['total_records'] = len(df)
        self.data_stats['features_count'] = len(df.columns)
        self.data_stats['price_range'] = (df['close'].min(), df['close'].max())
        
        self.logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        self.logger.info(f"üí∞ –î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω: ${df['close'].min():.2f} - ${df['close'].max():.2f}")
        
        return df
    
    def get_data_quality_report(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        report = {
            'total_records': len(df),
            'features_count': len(df.columns),
            'missing_values': df.isnull().sum().to_dict(),
            'price_statistics': {
                'min_price': df['close'].min(),
                'max_price': df['close'].max(),
                'mean_price': df['close'].mean(),
                'price_volatility': df['close'].std() / df['close'].mean()
            },
            'volume_statistics': {
                'min_volume': df['volume'].min(),
                'max_volume': df['volume'].max(),
                'mean_volume': df['volume'].mean()
            }
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
        price_anomalies = (
            (df['close'] > df['close'].quantile(0.99)) |
            (df['close'] < df['close'].quantile(0.01))
        ).sum()
        report['price_anomalies'] = price_anomalies
        
        return report


# –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
def load_and_prepare_data(file_path: str) -> pd.DataFrame:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    processor = DataProcessor()
    return processor.prepare_data(file_path) 