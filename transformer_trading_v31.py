"""
üöÄ TRANSFORMER –¢–û–†–ì–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê V3.1
–ó–∞–º–µ–Ω–∞ LSTM –Ω–∞ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å attention –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')


class TransformerConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Transformer —Å–∏—Å—Ç–µ–º—ã"""
    
    # Transformer –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    D_MODEL = 128          # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
    N_HEADS = 8            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ attention heads
    N_LAYERS = 3           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ transformer —Å–ª–æ—ë–≤
    D_FF = 512             # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å feed-forward —Å–µ—Ç–∏
    DROPOUT = 0.1          # Dropout rate
    
    # –¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    WINDOW_SIZE = 64       # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –¥–ª—è transformer
    INITIAL_BALANCE = 10000
    RISK_PER_TRADE = 0.02
    STOP_LOSS = 0.015
    TAKE_PROFIT = 0.045
    COMMISSION_RATE = 0.001
    
    # –û–±—É—á–µ–Ω–∏–µ
    TOTAL_TIMESTEPS = 50000
    LEARNING_RATE = 3e-4


class MultiHeadAttention(nn.Module):
    """Multi-Head Attention –º–µ—Ö–∞–Ω–∏–∑–º"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None
        
    def scaled_dot_product_attention(self, q, k, v, mask=None):
        """Scaled Dot-Product Attention"""
        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.attention_weights = attention_weights.detach()
        
        output = torch.matmul(attention_weights, v)
        return output
    
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.size()
        
        # Linear transformations
        q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        k = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        v = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # Attention
        attention_output = self.scaled_dot_product_attention(q, k, v, mask)
        
        # Concatenate heads
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        # Final linear transformation
        output = self.w_o(attention_output)
        return output


class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Transformer"""
    
    def __init__(self, d_model: int, max_len: int = 1000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class TransformerBlock(nn.Module):
    """Transformer –±–ª–æ–∫ —Å attention –∏ feed-forward"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # Multi-head attention with residual connection
        attention_output = self.attention(x, mask)
        x = self.norm1(x + self.dropout(attention_output))
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x


class TransformerFeatureExtractor(BaseFeaturesExtractor):
    """Transformer Feature Extractor –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, observation_space: gym.Space, features_dim: int = 256):
        super().__init__(observation_space, features_dim)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–∞
        if observation_space.shape is not None:
            self.seq_len = observation_space.shape[0]
            self.input_features = observation_space.shape[1]
        else:
            self.seq_len = TransformerConfig.WINDOW_SIZE
            self.input_features = 20
        
        self.d_model = TransformerConfig.D_MODEL
        
        # –í—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        self.input_projection = nn.Linear(self.input_features, self.d_model)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.pos_encoding = PositionalEncoding(self.d_model, max_len=self.seq_len)
        
        # Transformer –±–ª–æ–∫–∏
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(
                d_model=self.d_model,
                n_heads=TransformerConfig.N_HEADS,
                d_ff=TransformerConfig.D_FF,
                dropout=TransformerConfig.DROPOUT
            ) for _ in range(TransformerConfig.N_LAYERS)
        ])
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        self.output_projection = nn.Sequential(
            nn.Linear(self.d_model, features_dim),
            nn.ReLU(),
            nn.Dropout(TransformerConfig.DROPOUT),
            nn.Linear(features_dim, features_dim)
        )
        
        # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, features = observations.shape
        
        # –í—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        x = self.input_projection(observations)  # [batch, seq_len, d_model]
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        x = x.transpose(0, 1)  # [seq_len, batch, d_model]
        x = self.pos_encoding(x)
        x = x.transpose(0, 1)  # [batch, seq_len, d_model]
        
        # Transformer –±–ª–æ–∫–∏
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ pooling (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
        x = x.transpose(1, 2)  # [batch, d_model, seq_len]
        x = self.global_pool(x).squeeze(-1)  # [batch, d_model]
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        x = self.output_projection(x)
        
        return x


def generate_enhanced_crypto_data(n_points: int = 10000) -> pd.DataFrame:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Transformer —Å–∏—Å—Ç–µ–º—ã...")
    
    np.random.seed(42)
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    timestamps = pd.date_range(start='2020-01-01', periods=n_points, freq='1H')
    
    # –ë–∞–∑–æ–≤–∞—è —Ü–µ–Ω–∞ —Å —Ç—Ä–µ–Ω–¥–æ–º –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å—é
    trend = np.linspace(45000, 65000, n_points)
    seasonal = 2000 * np.sin(2 * np.pi * np.arange(n_points) / 168)  # –ù–µ–¥–µ–ª—å–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
    noise = np.random.normal(0, 1000, n_points)
    
    base_price = trend + seasonal + noise
    
    # –°–æ–∑–¥–∞–Ω–∏–µ OHLCV –¥–∞–Ω–Ω—ã—Ö
    df = pd.DataFrame({
        'timestamp': timestamps,
        'open': base_price,
        'high': base_price * (1 + np.abs(np.random.normal(0, 0.01, n_points))),
        'low': base_price * (1 - np.abs(np.random.normal(0, 0.01, n_points))),
        'close': base_price + np.random.normal(0, 500, n_points),
        'volume': np.random.exponential(1000000, n_points)
    })
    
    # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è high/low
    df['high'] = np.maximum(df['high'], df[['open', 'close']].max(axis=1))
    df['low'] = np.minimum(df['low'], df[['open', 'close']].min(axis=1))
    
    print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –¥–∞–Ω–Ω—ã—Ö")
    return df


def add_transformer_features(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è Transformer"""
    print("üîß –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['returns'] = df['close'].pct_change()
    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
    df['volatility'] = df['returns'].rolling(24).std()
    
    # Moving averages
    for period in [5, 10, 20, 50]:
        df[f'sma_{period}'] = df['close'].rolling(period).mean()
        df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
    
    # Technical indicators
    # RSI
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))
    
    # MACD
    exp1 = df['close'].ewm(span=12).mean()
    exp2 = df['close'].ewm(span=26).mean()
    df['macd'] = exp1 - exp2
    df['macd_signal'] = df['macd'].ewm(span=9).mean()
    df['macd_histogram'] = df['macd'] - df['macd_signal']
    
    # Bollinger Bands
    bb_period = 20
    bb_std = 2
    bb_sma = df['close'].rolling(bb_period).mean()
    bb_stddev = df['close'].rolling(bb_period).std()
    df['bb_upper'] = bb_sma + (bb_std * bb_stddev)
    df['bb_lower'] = bb_sma - (bb_std * bb_stddev)
    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / bb_sma
    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
    
    # Price patterns
    df['higher_high'] = (df['high'] > df['high'].shift(1)).astype(int)
    df['higher_low'] = (df['low'] > df['low'].shift(1)).astype(int)
    df['doji'] = (abs(df['close'] - df['open']) / (df['high'] - df['low']) < 0.1).astype(int)
    
    # Volume indicators
    df['volume_sma'] = df['volume'].rolling(20).mean()
    df['volume_ratio'] = df['volume'] / df['volume_sma']
    df['price_volume'] = df['close'] * df['volume']
    
    # Momentum indicators
    for period in [5, 10, 20]:
        df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
        df[f'roc_{period}'] = (df['close'] - df['close'].shift(period)) / df['close'].shift(period) * 100
    
    # Time-based features
    df['hour'] = df['timestamp'].dt.hour / 24.0
    df['day_of_week'] = df['timestamp'].dt.dayofweek / 7.0
    df['day_of_month'] = df['timestamp'].dt.day / 31.0
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É –∏ NaN
    df = df.drop(['timestamp'], axis=1)
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return df


class TransformerTradingEnv(gym.Env):
    """–¢–æ—Ä–≥–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
    
    def __init__(self, df: pd.DataFrame):
        super().__init__()
        
        self.df = df.reset_index(drop=True)
        self.window_size = TransformerConfig.WINDOW_SIZE
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: Hold, Buy, Sell
        self.action_space = spaces.Discrete(3)
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        n_features = len(df.columns)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(self.window_size, n_features),
            dtype=np.float32
        )
        
        self._reset_state()
    
    def _reset_state(self):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        self.current_step = self.window_size
        self.balance = TransformerConfig.INITIAL_BALANCE
        self.btc_amount = 0.0
        self.entry_price = 0.0
        
        self.total_trades = 0
        self.profitable_trades = 0
        self.portfolio_history = [TransformerConfig.INITIAL_BALANCE]
        self.trades_history = []
        
        # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ attention
        self.attention_history = []
    
    def reset(self, seed=None, options=None):
        self._reset_state()
        return self._get_observation(), {}
    
    def _get_observation(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –¥–ª—è Transformer"""
        start_idx = max(0, self.current_step - self.window_size)
        end_idx = self.current_step
        
        obs = self.df.iloc[start_idx:end_idx].values
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        if len(obs) < self.window_size:
            padding = np.tile(obs[0], (self.window_size - len(obs), 1))
            obs = np.vstack([padding, obs])
        
        return obs.astype(np.float32)
    
    def _get_current_price(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã"""
        if self.current_step >= len(self.df):
            return self.df.iloc[-1]['close']
        return self.df.iloc[self.current_step]['close']
    
    def _calculate_portfolio_value(self) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è"""
        current_price = self._get_current_price()
        return self.balance + self.btc_amount * current_price
    
    def _execute_trade(self, action: int, current_price: float) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        trade_result = {'executed': False, 'type': None, 'amount': 0, 'price': 0}
        
        if action == 1 and self.balance > 100:  # Buy
            investment = self.balance * TransformerConfig.RISK_PER_TRADE
            amount = investment / current_price
            commission = investment * TransformerConfig.COMMISSION_RATE
            
            self.btc_amount += amount
            self.balance -= investment + commission
            self.entry_price = current_price
            
            trade_result.update({
                'executed': True, 'type': 'BUY',
                'amount': amount, 'price': current_price,
                'investment': investment
            })
            
        elif action == 2 and self.btc_amount > 0:  # Sell
            revenue = self.btc_amount * current_price
            commission = revenue * TransformerConfig.COMMISSION_RATE
            
            profit = revenue - self.btc_amount * self.entry_price
            if profit > 0:
                self.profitable_trades += 1
            
            self.balance += revenue - commission
            self.btc_amount = 0.0
            self.entry_price = 0.0
            
            trade_result.update({
                'executed': True, 'type': 'SELL',
                'amount': self.btc_amount, 'price': current_price,
                'revenue': revenue, 'profit': profit
            })
        
        if trade_result['executed']:
            self.total_trades += 1
            self.trades_history.append(trade_result)
        
        return trade_result
    
    def _calculate_reward(self, current_price: float) -> float:
        """–†–∞—Å—á–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        current_portfolio = self._calculate_portfolio_value()
        prev_portfolio = self.portfolio_history[-1]
        
        # –ë–∞–∑–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞ - –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        portfolio_change = (current_portfolio - prev_portfolio) / prev_portfolio
        base_reward = portfolio_change * 100
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å—Ç–∞–±–∏–ª—å–Ω—É—é –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å
        if self.total_trades > 5:
            win_rate = self.profitable_trades / self.total_trades
            if win_rate > 0.6:
                base_reward *= 1.2
        
        return base_reward
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """–®–∞–≥ —Å–∏–º—É–ª—è—Ü–∏–∏"""
        current_price = self._get_current_price()
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        trade_result = self._execute_trade(action, current_price)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.current_step += 1
        portfolio_value = self._calculate_portfolio_value()
        self.portfolio_history.append(portfolio_value)
        
        # –†–∞—Å—á–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã
        reward = self._calculate_reward(current_price)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        done = (
            self.current_step >= len(self.df) - 1 or
            portfolio_value <= TransformerConfig.INITIAL_BALANCE * 0.1
        )
        
        info = {
            'portfolio_value': portfolio_value,
            'balance': self.balance,
            'btc_amount': self.btc_amount,
            'total_trades': self.total_trades,
            'trade_result': trade_result
        }
        
        return self._get_observation(), reward, done, False, info


def create_transformer_model(env):
    """–°–æ–∑–¥–∞–Ω–∏–µ PPO –º–æ–¥–µ–ª–∏ —Å Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    print("üß† –°–æ–∑–¥–∞–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ policy —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º feature extractor
    policy_kwargs = dict(
        features_extractor_class=TransformerFeatureExtractor,
        features_extractor_kwargs=dict(features_dim=256),
        net_arch=[256, 256],
        activation_fn=torch.nn.ReLU
    )
    
    model = PPO(
        'MlpPolicy',
        env,
        learning_rate=TransformerConfig.LEARNING_RATE,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        policy_kwargs=policy_kwargs,
        verbose=1
    )
    
    print("‚úÖ Transformer –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    return model


def train_transformer_model(model, total_timesteps: int = None):
    """–û–±—É—á–µ–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏"""
    if total_timesteps is None:
        total_timesteps = TransformerConfig.TOTAL_TIMESTEPS
    
    print(f"üéì –û–±—É—á–µ–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏ ({total_timesteps:,} —à–∞–≥–æ–≤)...")
    model.learn(total_timesteps=total_timesteps)
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return model


def test_transformer_model(model, env, max_steps: int = 2000):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏"""
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏ (–¥–æ {max_steps:,} —à–∞–≥–æ–≤)...")
    
    obs, _ = env.reset()
    results = []
    
    for step in range(max_steps):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = env.step(action)
        
        results.append({
            'step': step,
            'portfolio_value': info['portfolio_value'],
            'balance': info['balance'],
            'total_trades': info['total_trades'],
            'reward': reward
        })
        
        if done:
            break
    
    return results


def analyze_transformer_results(results: List[Dict], initial_balance: float):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Transformer —Å–∏—Å—Ç–µ–º—ã"""
    print("\nüìä –ê–ù–ê–õ–ò–ó TRANSFORMER –¢–û–†–ì–û–í–û–ô –°–ò–°–¢–ï–ú–´ V3.1")
    print("=" * 55)
    
    final_value = results[-1]['portfolio_value']
    total_return = (final_value - initial_balance) / initial_balance * 100
    total_trades = results[-1]['total_trades']
    
    print(f"üí∞ –ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å: {initial_balance:,.2f} USDT")
    print(f"üí∞ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: {final_value:,.2f} USDT")
    print(f"üìà –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {total_return:+.2f}%")
    print(f"üîÑ –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {total_trades}")
    print(f"üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Transformer (Multi-Head Attention)")
    print(f"üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {TransformerConfig.N_LAYERS} —Å–ª–æ—ë–≤, {TransformerConfig.N_HEADS} heads")
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    visualize_transformer_results(results)
    
    return {
        'total_return': total_return,
        'final_value': final_value,
        'total_trades': total_trades
    }


def visualize_transformer_results(results: List[Dict]):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Transformer —Å–∏—Å—Ç–µ–º—ã"""
    print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ Transformer –∞–Ω–∞–ª–∏–∑–∞...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('üöÄ Transformer Trading System V3.1 - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã', fontsize=16, fontweight='bold')
    
    steps = [r['step'] for r in results]
    portfolio_values = [r['portfolio_value'] for r in results]
    rewards = [r['reward'] for r in results]
    trades = [r['total_trades'] for r in results]
    
    # 1. –°—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è
    axes[0, 0].plot(steps, portfolio_values, linewidth=2, color='blue')
    axes[0, 0].set_title('üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å –ü–æ—Ä—Ç—Ñ–µ–ª—è')
    axes[0, 0].set_xlabel('–®–∞–≥–∏')
    axes[0, 0].set_ylabel('USDT')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã
    cumulative_rewards = np.cumsum(rewards)
    axes[0, 1].plot(steps, cumulative_rewards, linewidth=2, color='green')
    axes[0, 1].set_title('üèÜ –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –ù–∞–≥—Ä–∞–¥—ã')
    axes[0, 1].set_xlabel('–®–∞–≥–∏')
    axes[0, 1].set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫
    axes[1, 0].plot(steps, trades, linewidth=2, color='orange')
    axes[1, 0].set_title('üîÑ –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –°–¥–µ–ª–∫–∏')
    axes[1, 0].set_xlabel('–®–∞–≥–∏')
    axes[1, 0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. –°–≤–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    axes[1, 1].axis('off')
    summary_text = f"""
    üéØ TRANSFORMER –°–ò–°–¢–ï–ú–ê V3.1
    
    üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Multi-Head Attention
    üìè –°–ª–æ—ë–≤: {TransformerConfig.N_LAYERS}
    üëÅÔ∏è Attention Heads: {TransformerConfig.N_HEADS}
    ü™ü –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {TransformerConfig.WINDOW_SIZE}
    
    üí∞ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {portfolio_values[-1]:,.2f} USDT
    üìà –î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {(portfolio_values[-1]/portfolio_values[0]-1)*100:+.2f}%
    üîÑ –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {trades[-1]}
    
    ‚ú® –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Transformer:
    ‚Ä¢ –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    ‚Ä¢ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ attention –≤–µ—Å–∞
    ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU
    """
    axes[1, 1].text(0.1, 0.5, summary_text, transform=axes[1, 1].transAxes, 
                   fontsize=11, verticalalignment='center',
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.5))
    
    plt.tight_layout()
    plt.savefig('transformer_results_v31.png', dpi=300, bbox_inches='tight')
    print("üíæ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: transformer_results_v31.png")
    plt.show()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Transformer —Å–∏—Å—Ç–µ–º—ã V3.1"""
    print("üöÄ –ó–ê–ü–£–°–ö TRANSFORMER –¢–û–†–ì–û–í–û–ô –°–ò–°–¢–ï–ú–´ V3.1")
    print("=" * 60)
    
    # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –≠–¢–ê–ü 1: –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–ù–ù–´–•")
    print("-" * 30)
    df = generate_enhanced_crypto_data(n_points=8000)
    
    # 2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüîß –≠–¢–ê–ü 2: –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í")
    print("-" * 30)
    df = add_transformer_features(df)
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    print("\nüéÆ –≠–¢–ê–ü 3: –°–û–ó–î–ê–ù–ò–ï –û–ö–†–£–ñ–ï–ù–ò–Ø")
    print("-" * 30)
    env = TransformerTradingEnv(df)
    vec_env = DummyVecEnv([lambda: env])
    print(f"‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ —Å {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüß† –≠–¢–ê–ü 4: –°–û–ó–î–ê–ù–ò–ï –ò –û–ë–£–ß–ï–ù–ò–ï TRANSFORMER")
    print("-" * 30)
    model = create_transformer_model(vec_env)
    model = train_transformer_model(model, total_timesteps=30000)
    
    # 5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\nüß™ –≠–¢–ê–ü 5: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print("-" * 30)
    results = test_transformer_model(model, env, max_steps=2000)
    
    # 6. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –≠–¢–ê–ü 6: –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("-" * 30)
    analysis = analyze_transformer_results(results, TransformerConfig.INITIAL_BALANCE)
    
    # 7. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
    print("\nüéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï V3.1")
    print("=" * 50)
    print("üöÄ Transformer —Ç–æ—Ä–≥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ V3.1 —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞!")
    print(f"üí° –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {analysis['total_return']:+.2f}%")
    print(f"üß† –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Multi-Head Attention")
    print("\n‚ú® –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ V3.1:")
    print(f"  ‚Ä¢ {TransformerConfig.N_LAYERS} —Å–ª–æ—ë–≤ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã")
    print(f"  ‚Ä¢ {TransformerConfig.N_HEADS} attention heads –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    print(f"  ‚Ä¢ –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π ({TransformerConfig.WINDOW_SIZE})")
    print("  ‚Ä¢ –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("  ‚Ä¢ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print("  ‚Ä¢ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ attention –≤–µ—Å–∞")
    
    if analysis['total_return'] > 0:
        print("\nüü¢ –û–¶–ï–ù–ö–ê: –ü—Ä–∏–±—ã–ª—å–Ω–∞—è Transformer —Å—Ç—Ä–∞—Ç–µ–≥–∏—è!")
    else:
        print("\nüî∂ –û–¶–ï–ù–ö–ê: –¢—Ä–µ–±—É–µ—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
    
    print("\nüéâ –ê–ù–ê–õ–ò–ó V3.1 –ó–ê–í–ï–†–®–ï–ù!")


if __name__ == "__main__":
    main() 